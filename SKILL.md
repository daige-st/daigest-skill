---
name: daigest
description: Manage Daigest feeds - list, create, read, update, delete, sync AI-powered digests, and publish posts. Use when interacting with Daigest API.
allowed-tools: Bash
argument-hint: "[list | create | get <id> | sync <id> | posts <id>]"
---

# Daigest — Context Feed API for AI Agents

> Give your AI grounded context, not hallucinated search results.

Daigest delivers AI-summarized digests from trusted connections (RSS, web pages, Slack, Notion, GitHub, and more) — grounded in real content, always traceable to the original source. Agents can also create and manage their own feeds via this API when they need context on new topics.

When AI agents search the web, they risk hallucination and unreliable results. Daigest solves this: instead of searching and hoping for accuracy, the agent subscribes to trusted connections and gets pre-summarized, factual context on demand. No guesswork, no fabrication — just reliable information the agent can act on.

## Core Concepts

| Concept | Description |
|---------|-------------|
| **Feed** | The core unit. A container that holds connections, content, and memory. |
| **Connection** | An information channel to monitor — RSS feed, web page, Slack channel, etc. |
| **Memory** | Instructions for the AI: language, focus areas, output format, what to highlight or ignore. |
| **Content** | The feed's current markdown digest. Generated by sync or written directly by the agent. |
| **Post** | A published snapshot of content. Both sync and manual publishing create posts. |

## Authentication

```
Authorization: Bearer dk_live_xxxxxxxxxxxx
```

API keys: https://daige.st/account/api-keys

## Base URL & OpenAPI Spec

```
https://daige.st/api/v1
```

OpenAPI spec for auto-generating agent tools (Claude Code, Cursor, Windsurf, ChatGPT GPTs, Dify, n8n, etc.):

```
GET https://daige.st/api/v1/openapi.json
```

## Usage Flows

### A. Read existing feeds

The most common flow. Feeds are already set up (by a human in the web app or previously by an agent).

```
GET /feeds                              → find the relevant feed
GET /feeds/{id}?since=2026-02-09T00:00Z → check for updates
```

If `has_changes: true`, use the content. If `false`, skip — nothing new.

### B-1. Create a feed and get content via sync

```
POST /feeds
{
  "name": "AI News Digest",
  "memory": "Summarize the latest AI news. Highlight new model releases and benchmark results.",
  "connections": [
    { "type": "rss", "url": "https://example.com/ai-feed.xml" },
    { "type": "url", "url": "https://example.com/blog" }
  ]
}
→ 201 { "id": "feed-uuid", ... }
```

**Option A — Sync now:** Call sync immediately when the agent needs context right now.

```
POST /feeds/{id}/sync
→ 200 { "content": "# AI News\n\n...", "has_changes": true, ... }
```

Synchronous — takes 30s to 5min depending on connection count. Consumes AI quota.

**Option B — Schedule:** Add `schedule_enabled` and `scheduled_times` to auto-sync periodically, then poll for updates.

```
POST /feeds
{
  "name": "Daily Tech Brief",
  "memory": "Summarize key updates from these tech blogs.",
  "schedule_enabled": true,
  "scheduled_times": [{ "time": "09:00", "days": ["mon", "tue", "wed", "thu", "fri"] }],
  "connections": [{ "type": "rss", "url": "https://example.com/feed.xml" }]
}
→ 201 { "id": "feed-uuid", ... }

GET /feeds/{id}?since=2026-02-09T09:00:00Z
→ 200 { "has_changes": true, "content": "..." }   // new digest ready
```

### B-2. Create a feed and write content directly

The agent writes its own content to a feed — using Daigest as a context store without relying on AI sync.

```
POST /feeds { "name": "Research Notes" }
→ 201 { "id": "feed-uuid", ... }

PATCH /feeds/{id}
{ "content": "# Agent's research notes\n\n..." }
→ 200 { "content": "# Agent's research notes\n\n...", ... }
```

Useful when the agent curates and organizes information itself, and uses the feed to store or share the result.

### C. Adjust an existing feed

Use `PATCH /feeds/{id}` to change any combination of settings in a single request.

**Change connections and AI instructions, then re-sync:**

```
PATCH /feeds/{id}
{
  "memory": "Focus on product launches only. Ignore hiring news.",
  "add_connections": [{ "type": "url", "url": "https://example.com/changelog" }],
  "remove_connection_ids": ["old-connection-uuid"]
}

POST /feeds/{id}/sync    → regenerate with new settings
```

**Update content and publish as a post:**

```
PATCH /feeds/{id}
{ "content": "# Updated digest\n\nAgent-curated content here." }

POST /feeds/{id}/posts    → publish as a post (once per 15 min)
→ 201 { "version_number": 1, "content": "...", ... }
```

**Change schedule:**

```
PATCH /feeds/{id}
{
  "schedule_enabled": true,
  "scheduled_times": [{ "time": "18:00", "days": ["mon", "fri"] }]
}
```

## Endpoints

### List feeds

```
GET /feeds
→ 200 { "feeds": [{ "id", "name", "connection_count", "updated_at", ... }] }
```

### Create a feed

```
POST /feeds
{
  "name": "Feed Name",                          // required, max 200 chars
  "memory": "Instructions for AI summarizer",   // optional, max 1500 chars
  "schedule_enabled": true,                     // optional
  "scheduled_times": [{ "time": "09:00", "days": ["mon", "fri"] }],  // optional
  "connections": [{ "type": "rss", "url": "..." }] // optional
}
→ 201 { "id", "name", "schedule_enabled", "connection_count", "created_at" }
```

### Get feed details

```
GET /feeds/{id}
GET /feeds/{id}?since=2026-02-09T00:00:00Z
→ 200 {
  "id", "name", "schedule_enabled", "scheduled_times",
  "content",      // markdown, max 2000 chars. Empty when has_changes is false.
  "memory",       // AI instructions, max 1500 chars
  "has_changes",  // false if nothing changed since `since`
  "updated_at",
  "connections": [{ "id", "type", "name" }]
}
```

### Update a feed

```
PATCH /feeds/{id}
{
  "name": "New Name",                           // optional
  "content": "# Updated content",               // optional, max 2000 chars
  "memory": "New AI instructions",              // optional, max 1500 chars
  "schedule_enabled": true,                     // optional
  "scheduled_times": [{ "time": "10:00" }],     // optional
  "add_connections": [{ "type": "rss", "url": "..." }],   // optional, max 20
  "remove_connection_ids": ["connection-uuid"],            // optional, max 20
}
→ 200 { "id", "name", "content", "memory", "connections", "updated_at", ... }
```

- `remove_connection_ids`: Get IDs from `GET /feeds/{id}` response `connections[].id`.

### Delete a feed

```
DELETE /feeds/{id}
→ 200 { "deleted": true }
```

### Publish a post

Snapshot the current content as a new post. Rate limited to once per 15 minutes per feed.

```
POST /feeds/{id}/posts
→ 201 { "version_number", "name", "content", "references", "created_at" }
```

Returns 429 if called within 15 minutes of the last post.

### List past posts

Sync and publish both create posts. Use this to access historical digests.

```
GET /feeds/{id}/posts
GET /feeds/{id}/posts?cursor=42&limit=5
→ 200 {
  "posts": [{
    "version_number": 42,
    "name": "My Feed",
    "content": "# Summary...",
    "references": [{ "id": 1, "source_type": "rss", "source_id": "...", "title": "..." }],
    "created_at": "2026-02-08T09:00:00Z"
  }],
  "next_cursor": 38,
  "has_more": true
}
```

Paginate with `cursor` (version_number, exclusive) and `limit` (default 10, max 50).

### Get a specific past post

```
GET /feeds/{id}/posts/{versionNumber}
→ 200 { "version_number", "name", "content", "references", "created_at" }
```

Returns 404 if the version doesn't exist.

### Sync a feed

```
POST /feeds/{id}/sync
→ 200 { "id", "name", "content", "has_changes": true, "connections", ... }
```

Fetches latest data from all connections, then AI generates/updates the digest. Synchronous — waits until completion. Consumes AI quota.

## Connection Types

### Create via API

| Type | Description |
|------|-------------|
| `rss` | Any RSS or Atom feed URL |
| `url` | Any web page — Daigest extracts and monitors the content |

### Configure in web app, use via API

These connections require setup in the [Daigest web app](https://daige.st) first, then you can read and sync feeds containing them via the API.

| Type | Setup |
|------|-------|
| `slack` | OAuth |
| `notion` | OAuth |
| `github` | OAuth |
| `discord` | OAuth (Pro plan) |
| `youtube` | Web app (Pro plan) |
| `x` | Web app (Pro plan) |
| `reddit` | Web app (Pro plan) |
| `substack` | Web app (Pro plan) |
| `threads` | Web app (Pro plan) |

## Best Practices

### Memory examples

- **Language**: `"Write the digest in Korean."` or `"Summarize in Japanese with English technical terms."`
- **Focus**: `"Focus on pricing changes and product launches. Ignore job postings."`
- **Format**: `"Use bullet points grouped by topic. Start each section with a one-line summary."`
- **Tone**: `"Write for a technical audience. Be concise — no fluff."`

### Sync cost management

Each sync consumes an AI request from the user's plan.
- Prefer scheduled syncs (`schedule_enabled`) over frequent manual syncs
- Combine related connections into one feed rather than many small feeds
- Use `since` to check for changes before deciding to sync

### Connection combination

RSS for regular updates + URL for specific pages gives comprehensive coverage. For example, an RSS feed for blog posts + a URL for the changelog page.

## Rate Limits

| Scope | Limit |
|-------|-------|
| General | 60 req/min per API key |
| Sync | 10 req/min per API key |

Returns `429` with `Retry-After` header (seconds) when exceeded.

## Error Responses

```json
{ "error": "description", "code": "ERROR_CODE" }
```

| Status | Meaning |
|--------|---------|
| 400 | Validation error |
| 401 | Missing or invalid API key |
| 404 | Feed not found |
| 429 | Rate limit exceeded |

## Links

- Website: https://daige.st
- OpenAPI spec: https://daige.st/api/v1/openapi.json
- API keys: https://daige.st/account/api-keys
- Support: help@daige.st
